{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdAO4aOGW0Qa5TZyugz14z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imranajec/MetaData-on-Chicago-factory-Dataset/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ml_metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RgnDOvufaQ9",
        "outputId": "5dc2b36a-5024-48b5-9590-bb5fd98b0be1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ml_metadata\n",
            "  Downloading ml_metadata-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from ml_metadata) (1.4.0)\n",
            "Collecting attrs<22,>=20.3 (from ml_metadata)\n",
            "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio<2,>=1.8.6 in /usr/local/lib/python3.10/dist-packages (from ml_metadata) (1.62.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.13 in /usr/local/lib/python3.10/dist-packages (from ml_metadata) (3.20.3)\n",
            "Requirement already satisfied: six<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from ml_metadata) (1.16.0)\n",
            "Installing collected packages: attrs, ml_metadata\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.2.0\n",
            "    Uninstalling attrs-23.2.0:\n",
            "      Successfully uninstalled attrs-23.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jsonschema 4.19.2 requires attrs>=22.2.0, but you have attrs 21.4.0 which is incompatible.\n",
            "referencing 0.33.0 requires attrs>=22.2.0, but you have attrs 21.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-21.4.0 ml_metadata-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_data_validation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nT3QPCB4fcMn",
        "outputId": "251ca560-31df-45bd-b157-1952e3f50488"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_data_validation\n",
            "  Downloading tensorflow_data_validation-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow_data_validation) (1.4.0)\n",
            "Collecting apache-beam[gcp]<3,>=2.47 (from tensorflow_data_validation)\n",
            "  Downloading apache_beam-2.54.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_data_validation) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_data_validation) (1.25.2)\n",
            "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_data_validation) (1.5.3)\n",
            "Requirement already satisfied: protobuf<5,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_data_validation) (3.20.3)\n",
            "Collecting pyarrow<11,>=10 (from tensorflow_data_validation)\n",
            "  Downloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyfarmhash<0.4,>=0.2.2 (from tensorflow_data_validation)\n",
            "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six<2,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow_data_validation) (1.16.0)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow_data_validation) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-metadata<1.15,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_data_validation) (1.14.0)\n",
            "Collecting tfx-bsl<1.15,>=1.14.0 (from tensorflow_data_validation)\n",
            "  Downloading tfx_bsl-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting crcmod<2.0,>=1.7 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting orjson<4,>=3.9.7 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.2.1)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners<1.0,>=0.3 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (1.62.1)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.22.0)\n",
            "Collecting js2py<1,>=0.74 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (4.19.2)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (3.0.3)\n",
            "Collecting numpy>=1.22.0 (from tensorflow_data_validation)\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting objsize<0.8.0,>=0.6.1 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading objsize-0.7.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (24.0)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading pymongo-4.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.2/677.2 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (1.23.0)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2023.4)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2023.12.25)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (4.10.0)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.6)\n",
            "Requirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (5.3.3)\n",
            "Requirement already satisfied: google-api-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.11.1)\n",
            "Collecting google-apitools<0.5.32,>=0.5.31 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.1.1)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.15.2)\n",
            "Collecting google-cloud-pubsub<3,>=2.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_pubsub-2.20.3-py2.py3-none-any.whl (273 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.0/273.0 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-pubsublite<2,>=1.2.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_pubsublite-1.9.0-py2.py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-storage<3,>=2.14.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_storage-2.16.0-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (3.12.0)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.24.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.3.3)\n",
            "Collecting google-cloud-bigtable<3,>=2.19.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_bigtable-2.23.0-py2.py3-none-any.whl (357 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.5/357.5 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-spanner<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_spanner-3.44.0-py2.py3-none-any.whl (357 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-dlp<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_dlp-3.16.0-py2.py3-none-any.whl (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.13.3)\n",
            "Collecting google-cloud-videointelligence<3,>=2.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_videointelligence-2.13.3-py2.py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.4/240.4 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-vision<4,>=2 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_vision-3.7.2-py2.py3-none-any.whl (459 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.6/459.6 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-recommendations-ai<0.11.0,>=0.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_cloud_recommendations_ai-0.10.10-py2.py3-none-any.whl (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (1.44.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_data_validation) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_data_validation) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_data_validation) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_data_validation) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_data_validation) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_data_validation) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_data_validation) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_data_validation) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_data_validation) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_data_validation) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_data_validation) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_data_validation) (0.36.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_data_validation) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_data_validation) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_data_validation) (2.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata<1.15,>=1.14.0->tensorflow_data_validation) (1.63.0)\n",
            "Collecting google-api-python-client<2,>=1.7.11 (from tfx-bsl<1.15,>=1.14.0->tensorflow_data_validation)\n",
            "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-serving-api<3,>=2.13.0 (from tfx-bsl<1.15,>=1.14.0->tensorflow_data_validation)\n",
            "  Downloading tensorflow_serving_api-2.14.1-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<3,>=2.13->tensorflow_data_validation) (0.43.0)\n",
            "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.15,>=1.14.0->tensorflow_data_validation)\n",
            "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (4.1.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (4.9)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (1.12.3)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.0.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.7.0)\n",
            "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading google_api_core-2.18.0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.13.0)\n",
            "Requirement already satisfied: grpcio-status>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (1.48.2)\n",
            "Collecting overrides<8.0.0,>=6.0.1 (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.4.4)\n",
            "Collecting deprecated>=1.2.14 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting grpc-interceptor>=0.15.4 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading grpc_interceptor-0.15.4-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.14.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (1.5.0)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (3.1.2)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (5.2)\n",
            "Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting attrs>=22.2.0 (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.18.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (2024.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13->tensorflow_data_validation) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13->tensorflow_data_validation) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13->tensorflow_data_validation) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13->tensorflow_data_validation) (3.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13->tensorflow_data_validation) (1.4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13->tensorflow_data_validation) (2.1.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13->tensorflow_data_validation) (3.2.2)\n",
            "Building wheels for collected packages: pyfarmhash, crcmod, dill, google-apitools, hdfs, pyjsparser, docopt\n",
            "  Building wheel for pyfarmhash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp310-cp310-linux_x86_64.whl size=88655 sha256=daf128ce5dae071e86a3867fe2095a9454842906bc8a08f25ab768cb05a350a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/08/da/f66b1f3258fe3f1e767b2136c5444dbfa9fa3f7944cc5e1983\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31406 sha256=449a1bf7409c36ba49e48a2d440215f62338bd65dc7fdefe42a92d9fa6dfde7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78541 sha256=3aee10e047e424bc12eeb38dbc931c6cb16198671a3397cd09f06b98a043309f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131015 sha256=d0d5db9d0ad5c053845c8c742cca1bbd48ab98eed076bdf6d76ed0374b896829\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/b7/e0/9712f8c23a5da3d9d16fb88216b897bf60e85b12f5470f26ee\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34324 sha256=239d0ec1bea50ffc6c69a602017e865372af44e40a17be4ac108de88386d52a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25984 sha256=975cd6e8eefd0a6a82bd9bc8ff1779076459f91266a1eea89f1dc2cd42e33b5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=5e7121f9764e0ef3d73af33ebf69059d1226535262e72f285c4fd694ab7dada5\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built pyfarmhash crcmod dill google-apitools hdfs pyjsparser docopt\n",
            "Installing collected packages: pyjsparser, pyfarmhash, docopt, crcmod, zstandard, uritemplate, overrides, orjson, objsize, numpy, js2py, grpc-interceptor, fasteners, fastavro, dnspython, dill, deprecated, attrs, pymongo, pyarrow, hdfs, google-apitools, google-api-core, google-api-python-client, google-cloud-vision, google-cloud-videointelligence, google-cloud-storage, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-dlp, google-cloud-bigtable, apache-beam, tensorflow-serving-api, google-cloud-pubsublite, tfx-bsl, tensorflow_data_validation\n",
            "  Attempting uninstall: uritemplate\n",
            "    Found existing installation: uritemplate 4.1.1\n",
            "    Uninstalling uritemplate-4.1.1:\n",
            "      Successfully uninstalled uritemplate-4.1.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 21.4.0\n",
            "    Uninstalling attrs-21.4.0:\n",
            "      Successfully uninstalled attrs-21.4.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 2.11.1\n",
            "    Uninstalling google-api-core-2.11.1:\n",
            "      Successfully uninstalled google-api-core-2.11.1\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.84.0\n",
            "    Uninstalling google-api-python-client-2.84.0:\n",
            "      Successfully uninstalled google-api-python-client-2.84.0\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 2.8.0\n",
            "    Uninstalling google-cloud-storage-2.8.0:\n",
            "      Successfully uninstalled google-cloud-storage-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ml-metadata 1.14.0 requires attrs<22,>=20.3, but you have attrs 23.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed apache-beam-2.54.0 attrs-23.2.0 crcmod-1.7 deprecated-1.2.14 dill-0.3.1.1 dnspython-2.6.1 docopt-0.6.2 fastavro-1.9.4 fasteners-0.19 google-api-core-2.18.0 google-api-python-client-1.12.11 google-apitools-0.5.31 google-cloud-bigtable-2.23.0 google-cloud-dlp-3.16.0 google-cloud-pubsub-2.20.3 google-cloud-pubsublite-1.9.0 google-cloud-recommendations-ai-0.10.10 google-cloud-spanner-3.44.0 google-cloud-storage-2.16.0 google-cloud-videointelligence-2.13.3 google-cloud-vision-3.7.2 grpc-interceptor-0.15.4 hdfs-2.7.3 js2py-0.74 numpy-1.24.4 objsize-0.7.0 orjson-3.9.15 overrides-7.7.0 pyarrow-10.0.1 pyfarmhash-0.3.2 pyjsparser-2.7.1 pymongo-4.6.2 tensorflow-serving-api-2.14.1 tensorflow_data_validation-1.14.0 tfx-bsl-1.14.0 uritemplate-3.0.1 zstandard-0.22.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "googleapiclient",
                  "numpy",
                  "pyarrow",
                  "uritemplate"
                ]
              },
              "id": "e08dfb2ababe42899a8b21cdf7378498"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_e2u16TfU9h",
        "outputId": "0b2c75ff-b58b-4cf5-aba5-cb8000e417ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.15.0\n",
            "TFDV version: 1.14.0\n"
          ]
        }
      ],
      "source": [
        "from ml_metadata.metadata_store import metadata_store\n",
        "from ml_metadata.proto import metadata_store_pb2\n",
        "\n",
        "import tensorflow as tf\n",
        "print('TF version: {}'.format(tf.__version__))\n",
        "\n",
        "import tensorflow_data_validation as tfdv\n",
        "print('TFDV version: {}'.format(tfdv.version.__version__))\n",
        "\n",
        "import urllib\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the zip file from GCP and unzip it\n",
        "url = 'https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/chicago_data.zip'\n",
        "zip, headers = urllib.request.urlretrieve(url)\n",
        "zipfile.ZipFile(zip).extractall()\n",
        "zipfile.ZipFile(zip).close()\n",
        "\n",
        "print(\"Here's what we downloaded:\")\n",
        "!ls -R data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM6UmONNfVhK",
        "outputId": "a3965e90-e28d-454c-bcc7-6d8720b7355d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's what we downloaded:\n",
            "data:\n",
            "eval  serving  train\n",
            "\n",
            "data/eval:\n",
            "data.csv\n",
            "\n",
            "data/serving:\n",
            "data.csv\n",
            "\n",
            "data/train:\n",
            "data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a connection config\n",
        "connection_config = metadata_store_pb2.ConnectionConfig()\n",
        "\n",
        "# Set an empty fake database proto\n",
        "connection_config.fake_database.SetInParent()\n",
        "\n",
        "# Setup the metadata store\n",
        "store = metadata_store.MetadataStore(connection_config)"
      ],
      "metadata": {
        "id": "LAxy5joZgEnZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ArtifactType for the input dataset\n",
        "data_artifact_type = metadata_store_pb2.ArtifactType()\n",
        "data_artifact_type.name = 'DataSet'\n",
        "data_artifact_type.properties['name'] = metadata_store_pb2.STRING\n",
        "data_artifact_type.properties['split'] = metadata_store_pb2.STRING\n",
        "data_artifact_type.properties['version'] = metadata_store_pb2.INT\n",
        "\n",
        "# Register artifact type to the Metadata Store\n",
        "data_artifact_type_id = store.put_artifact_type(data_artifact_type)\n",
        "\n",
        "# Create ArtifactType for Schema\n",
        "schema_artifact_type = metadata_store_pb2.ArtifactType()\n",
        "schema_artifact_type.name = 'Schema'\n",
        "schema_artifact_type.properties['name'] = metadata_store_pb2.STRING\n",
        "schema_artifact_type.properties['version'] = metadata_store_pb2.INT\n",
        "\n",
        "# Register artifact type to the Metadata Store\n",
        "schema_artifact_type_id = store.put_artifact_type(schema_artifact_type)\n",
        "\n",
        "print('Data artifact type:\\n', data_artifact_type)\n",
        "print('Schema artifact type:\\n', schema_artifact_type)\n",
        "print('Data artifact type ID:', data_artifact_type_id)\n",
        "print('Schema artifact type ID:', schema_artifact_type_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpQWwiPNgIFp",
        "outputId": "236a4187-f9d2-4916-f4bb-8511c2f6b1e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data artifact type:\n",
            " name: \"DataSet\"\n",
            "properties {\n",
            "  key: \"name\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"split\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "\n",
            "Schema artifact type:\n",
            " name: \"Schema\"\n",
            "properties {\n",
            "  key: \"name\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "\n",
            "Data artifact type ID: 10\n",
            "Schema artifact type ID: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ExecutionType for Data Validation component\n",
        "dv_execution_type = metadata_store_pb2.ExecutionType()\n",
        "dv_execution_type.name = 'Data Validation'\n",
        "dv_execution_type.properties['state'] = metadata_store_pb2.STRING\n",
        "\n",
        "# Register execution type to the Metadata Store\n",
        "dv_execution_type_id = store.put_execution_type(dv_execution_type)\n",
        "\n",
        "print('Data validation execution type:\\n', dv_execution_type)\n",
        "print('Data validation execution type ID:', dv_execution_type_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZVQFoLFgKkQ",
        "outputId": "f0ca611e-0bdf-4be6-81fb-473505489aba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data validation execution type:\n",
            " name: \"Data Validation\"\n",
            "properties {\n",
            "  key: \"state\"\n",
            "  value: STRING\n",
            "}\n",
            "\n",
            "Data validation execution type ID: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare input artifact of type DataSet\n",
        "data_artifact = metadata_store_pb2.Artifact()\n",
        "data_artifact.uri = './data/train/data.csv'\n",
        "data_artifact.type_id = data_artifact_type_id\n",
        "data_artifact.properties['name'].string_value = 'Chicago Taxi dataset'\n",
        "data_artifact.properties['split'].string_value = 'train'\n",
        "data_artifact.properties['version'].int_value = 1\n",
        "\n",
        "# Submit input artifact to the Metadata Store\n",
        "data_artifact_id = store.put_artifacts([data_artifact])[0]\n",
        "\n",
        "print('Data artifact:\\n', data_artifact)\n",
        "print('Data artifact ID:', data_artifact_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dahQYcDzgNLW",
        "outputId": "ce6381d3-fd9d-48ab-b425-2b2c01fb6592"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data artifact:\n",
            " type_id: 10\n",
            "uri: \"./data/train/data.csv\"\n",
            "properties {\n",
            "  key: \"name\"\n",
            "  value {\n",
            "    string_value: \"Chicago Taxi dataset\"\n",
            "  }\n",
            "}\n",
            "properties {\n",
            "  key: \"split\"\n",
            "  value {\n",
            "    string_value: \"train\"\n",
            "  }\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value {\n",
            "    int_value: 1\n",
            "  }\n",
            "}\n",
            "\n",
            "Data artifact ID: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the Execution of a Data Validation run\n",
        "dv_execution = metadata_store_pb2.Execution()\n",
        "dv_execution.type_id = dv_execution_type_id\n",
        "dv_execution.properties['state'].string_value = 'RUNNING'\n",
        "\n",
        "# Submit execution unit to the Metadata Store\n",
        "dv_execution_id = store.put_executions([dv_execution])[0]\n",
        "\n",
        "print('Data validation execution:\\n', dv_execution)\n",
        "print('Data validation execution ID:', dv_execution_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNcx-4FOgPJb",
        "outputId": "ceb902fa-e487-424e-c7ea-e8f981d4e987"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data validation execution:\n",
            " type_id: 12\n",
            "properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"RUNNING\"\n",
            "  }\n",
            "}\n",
            "\n",
            "Data validation execution ID: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare the input event\n",
        "input_event = metadata_store_pb2.Event()\n",
        "input_event.artifact_id = data_artifact_id\n",
        "input_event.execution_id = dv_execution_id\n",
        "input_event.type = metadata_store_pb2.Event.DECLARED_INPUT\n",
        "\n",
        "# Submit input event to the Metadata Store\n",
        "store.put_events([input_event])\n",
        "\n",
        "print('Input event:\\n', input_event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WaoevDXgRjU",
        "outputId": "3434d181-79f6-4fe4-a2ea-cfb1494e141c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input event:\n",
            " artifact_id: 1\n",
            "execution_id: 1\n",
            "type: DECLARED_INPUT\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Infer a schema by passing statistics to `infer_schema()`\n",
        "train_data = './data/train/data.csv'\n",
        "train_stats = tfdv.generate_statistics_from_csv(data_location=train_data)\n",
        "schema = tfdv.infer_schema(statistics=train_stats)\n",
        "\n",
        "schema_file = './schema.pbtxt'\n",
        "tfdv.write_schema_text(schema, schema_file)\n",
        "\n",
        "print(\"Dataset's Schema has been generated at:\", schema_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "nqAw73MIgUXl",
        "outputId": "869d4a3e-7deb-4de9-bdae-11c010019b05"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_data_validation/utils/artifacts_io_impl.py:93: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset's Schema has been generated at: ./schema.pbtxt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare output artifact of type Schema_artifact\n",
        "schema_artifact = metadata_store_pb2.Artifact()\n",
        "schema_artifact.uri = schema_file\n",
        "schema_artifact.type_id = schema_artifact_type_id\n",
        "schema_artifact.properties['version'].int_value = 1\n",
        "schema_artifact.properties['name'].string_value = 'Chicago Taxi Schema'\n",
        "\n",
        "# Submit output artifact to the Metadata Store\n",
        "schema_artifact_id = store.put_artifacts([schema_artifact])[0]\n",
        "\n",
        "print('Schema artifact:\\n', schema_artifact)\n",
        "print('Schema artifact ID:', schema_artifact_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhce42mPgXA7",
        "outputId": "6de1473b-6bd4-4841-ec2e-4a8c9134d0b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema artifact:\n",
            " type_id: 11\n",
            "uri: \"./schema.pbtxt\"\n",
            "properties {\n",
            "  key: \"name\"\n",
            "  value {\n",
            "    string_value: \"Chicago Taxi Schema\"\n",
            "  }\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value {\n",
            "    int_value: 1\n",
            "  }\n",
            "}\n",
            "\n",
            "Schema artifact ID: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare the output event\n",
        "output_event = metadata_store_pb2.Event()\n",
        "output_event.artifact_id = schema_artifact_id\n",
        "output_event.execution_id = dv_execution_id\n",
        "output_event.type = metadata_store_pb2.Event.DECLARED_OUTPUT\n",
        "\n",
        "# Submit output event to the Metadata Store\n",
        "store.put_events([output_event])\n",
        "\n",
        "print('Output event:\\n', output_event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEzvi9FSghMY",
        "outputId": "a54acd44-7a95-4ebd-ff59-05addba77749"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output event:\n",
            " artifact_id: 2\n",
            "execution_id: 1\n",
            "type: DECLARED_OUTPUT\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mark the `state` as `COMPLETED`\n",
        "dv_execution.id = dv_execution_id\n",
        "dv_execution.properties['state'].string_value = 'COMPLETED'\n",
        "\n",
        "# Update execution unit in the Metadata Store\n",
        "store.put_executions([dv_execution])\n",
        "\n",
        "print('Data validation execution:\\n', dv_execution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8WXvrJZgj5b",
        "outputId": "947e262c-de7f-41cd-bd9e-3aeb586fffdc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data validation execution:\n",
            " id: 1\n",
            "type_id: 12\n",
            "properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"COMPLETED\"\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ContextType\n",
        "expt_context_type = metadata_store_pb2.ContextType()\n",
        "expt_context_type.name = 'Experiment'\n",
        "expt_context_type.properties['note'] = metadata_store_pb2.STRING\n",
        "\n",
        "# Register context type to the Metadata Store\n",
        "expt_context_type_id = store.put_context_type(expt_context_type)"
      ],
      "metadata": {
        "id": "xwp_liJQglxG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the context\n",
        "expt_context = metadata_store_pb2.Context()\n",
        "expt_context.type_id = expt_context_type_id\n",
        "# Give the experiment a name\n",
        "expt_context.name = 'Demo'\n",
        "expt_context.properties['note'].string_value = 'Walkthrough of metadata'\n",
        "\n",
        "# Submit context to the Metadata Store\n",
        "expt_context_id = store.put_contexts([expt_context])[0]\n",
        "\n",
        "print('Experiment Context type:\\n', expt_context_type)\n",
        "print('Experiment Context type ID: ', expt_context_type_id)\n",
        "\n",
        "print('Experiment Context:\\n', expt_context)\n",
        "print('Experiment Context ID: ', expt_context_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwUVmhZ_go_a",
        "outputId": "b9b46551-e29c-4bb0-e215-0ea78f022e31"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment Context type:\n",
            " name: \"Experiment\"\n",
            "properties {\n",
            "  key: \"note\"\n",
            "  value: STRING\n",
            "}\n",
            "\n",
            "Experiment Context type ID:  13\n",
            "Experiment Context:\n",
            " type_id: 13\n",
            "name: \"Demo\"\n",
            "properties {\n",
            "  key: \"note\"\n",
            "  value {\n",
            "    string_value: \"Walkthrough of metadata\"\n",
            "  }\n",
            "}\n",
            "\n",
            "Experiment Context ID:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the attribution\n",
        "expt_attribution = metadata_store_pb2.Attribution()\n",
        "expt_attribution.artifact_id = schema_artifact_id\n",
        "expt_attribution.context_id = expt_context_id\n",
        "\n",
        "# Generate the association\n",
        "expt_association = metadata_store_pb2.Association()\n",
        "expt_association.execution_id = dv_execution_id\n",
        "expt_association.context_id = expt_context_id\n",
        "\n",
        "# Submit attribution and association to the Metadata Store\n",
        "store.put_attributions_and_associations([expt_attribution], [expt_association])\n",
        "\n",
        "print('Experiment Attribution:\\n', expt_attribution)\n",
        "print('Experiment Association:\\n', expt_association)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNECV342gquY",
        "outputId": "bf2fb4f7-a019-41c9-ea0c-6420d5f0f30b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment Attribution:\n",
            " artifact_id: 2\n",
            "context_id: 1\n",
            "\n",
            "Experiment Association:\n",
            " execution_id: 1\n",
            "context_id: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get artifact types\n",
        "store.get_artifact_types()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu_bpo1bgtRr",
        "outputId": "e8025eb1-5065-473a-a08c-effc2018e6af"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[id: 10\n",
              " name: \"DataSet\"\n",
              " properties {\n",
              "   key: \"name\"\n",
              "   value: STRING\n",
              " }\n",
              " properties {\n",
              "   key: \"split\"\n",
              "   value: STRING\n",
              " }\n",
              " properties {\n",
              "   key: \"version\"\n",
              "   value: INT\n",
              " },\n",
              " id: 11\n",
              " name: \"Schema\"\n",
              " properties {\n",
              "   key: \"name\"\n",
              "   value: STRING\n",
              " }\n",
              " properties {\n",
              "   key: \"version\"\n",
              "   value: INT\n",
              " }]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 1st element in the list of `Schema` artifacts.\n",
        "# You will investigate which dataset was used to generate it.\n",
        "schema_to_inv = store.get_artifacts_by_type('Schema')[0]\n",
        "\n",
        "# print output\n",
        "print(schema_to_inv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-jx6Q5Mgvp9",
        "outputId": "7ae126bd-a72d-49af-ed30-06355aad5482"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id: 2\n",
            "type_id: 11\n",
            "uri: \"./schema.pbtxt\"\n",
            "properties {\n",
            "  key: \"name\"\n",
            "  value {\n",
            "    string_value: \"Chicago Taxi Schema\"\n",
            "  }\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value {\n",
            "    int_value: 1\n",
            "  }\n",
            "}\n",
            "type: \"Schema\"\n",
            "create_time_since_epoch: 1711066453445\n",
            "last_update_time_since_epoch: 1711066453445\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get events related to the schema id\n",
        "schema_events = store.get_events_by_artifact_ids([schema_to_inv.id])\n",
        "\n",
        "print(schema_events)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CbrQNuugyuP",
        "outputId": "c6458422-64c0-4d07-9714-d4dc742308b6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[artifact_id: 2\n",
            "execution_id: 1\n",
            "type: DECLARED_OUTPUT\n",
            "milliseconds_since_epoch: 1711066464467\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get events related to the output above\n",
        "execution_events = store.get_events_by_execution_ids([schema_events[0].execution_id])\n",
        "\n",
        "print(execution_events)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoXqE5x1g8DF",
        "outputId": "7efb84b9-0f25-4edb-a31e-f083a0492f45"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[artifact_id: 1\n",
            "execution_id: 1\n",
            "type: DECLARED_INPUT\n",
            "milliseconds_since_epoch: 1711066401131\n",
            ", artifact_id: 2\n",
            "execution_id: 1\n",
            "type: DECLARED_OUTPUT\n",
            "milliseconds_since_epoch: 1711066464467\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look up the artifact that is a declared input\n",
        "artifact_input = execution_events[0]\n",
        "\n",
        "store.get_artifacts_by_id([artifact_input.artifact_id])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV7Ikz2tg_DV",
        "outputId": "e17bbbc7-19ac-470e-ec2b-ff8e26906d1a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[id: 1\n",
              " type_id: 10\n",
              " uri: \"./data/train/data.csv\"\n",
              " properties {\n",
              "   key: \"name\"\n",
              "   value {\n",
              "     string_value: \"Chicago Taxi dataset\"\n",
              "   }\n",
              " }\n",
              " properties {\n",
              "   key: \"split\"\n",
              "   value {\n",
              "     string_value: \"train\"\n",
              "   }\n",
              " }\n",
              " properties {\n",
              "   key: \"version\"\n",
              "   value {\n",
              "     int_value: 1\n",
              "   }\n",
              " }\n",
              " type: \"DataSet\"\n",
              " create_time_since_epoch: 1711066379481\n",
              " last_update_time_since_epoch: 1711066379481]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mf9_gqFShDZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}